{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from prepare_input import prepare_input_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "# Enable Eager Execution\n",
    "tf.enable_eager_execution() #kind of useless for the estimator\n",
    "\n",
    "# Set Verbosity\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "BLOCK_SIZE = 100\n",
    "\n",
    "\n",
    "# Load training and eval data\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    data_dir = 'Training_Out'\n",
    "    #seed = 5\n",
    "    num_experiments = 20\n",
    "    num_epochs = 1  # Number of times we go over all the samples in train_data\n",
    "\n",
    "\n",
    "    for seed in range(num_experiments):\n",
    "        ((train_data, train_labels, train_times), (eval_data, eval_labels, eval_times)) = prepare_input_dataset(seed, data_dir)\n",
    "        print(train_data.shape, train_labels.shape)\n",
    "        print(eval_data.shape, eval_labels.shape)\n",
    "\n",
    "        checkpoint_path = \"lstm_model_training/cp-{experiment:04d}.ckpt\"\n",
    "        #checkpoint_path = \"lstm_model_training/cp-{experiment:04d}-{epoch:04d}.ckpt\"\n",
    "        #checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "\n",
    "        inputs = tf.keras.Input(shape=(None,2) )\n",
    "        #lstm_1 = tf.keras.layers.LSTM(32, return_sequences=True)(inputs)\n",
    "        lstm = tf.keras.layers.LSTM(256)(inputs)\n",
    "        outputs = tf.keras.layers.Dense(3, activation='softmax')(lstm)\n",
    "\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        # Specify the training configuration (optimizer, loss, metrics)\n",
    "        model.compile(optimizer=tf.keras.optimizers.SGD(0.01), #learning rate 0.01  # Optimizer\n",
    "                    # Loss function to minimize\n",
    "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                    # List of metrics to monitor\n",
    "                    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "        model.summary()\n",
    "        #model.save_weights(checkpoint_path.format(experiment=seed, epoch=0))\n",
    "        model.save_weights(checkpoint_path.format(experiment=seed))\n",
    "\n",
    "\n",
    "        ttrue = []\n",
    "        tpred = []\n",
    "        for e in range(num_epochs):\n",
    "            ytrue = []\n",
    "            ypred = []\n",
    "            for n, i in enumerate(np.random.permutation(train_data.shape[0])):\n",
    "\n",
    "                #model.load_weights(checkpoint_path.format(experiment=seed, epoch=e))\n",
    "                model.load_weights(checkpoint_path.format(experiment=seed))\n",
    "                # Train the model\n",
    "                batch=100 #alternative to num_epochs, repeat sample batch number of times\n",
    "                x = np.repeat(np.expand_dims(train_data[i], axis=0), batch, axis=0)\n",
    "                y = np.repeat(train_labels[i:i+1], batch, axis=0)\n",
    "                print(\"Exp %d epoch %d -- %d / %d \" % (seed, e+1, n+1, train_data.shape[0]))\n",
    "                history = model.fit(x[:, :500, :], y,   # use [:, :500, :] to take first 500 steps in the sequence, since it's too big.\n",
    "                        batch_size=batch,\n",
    "                        epochs=1,\n",
    "                        verbose=0)\n",
    "                #model.save_weights(checkpoint_path.format(experiment=seed, epoch=e+1))\n",
    "                model.save_weights(checkpoint_path.format(experiment=seed))\n",
    "                #print('# save model')\n",
    "\n",
    "            for i in range(train_data.shape[0]):\n",
    "\n",
    "                predictions = model.predict(np.expand_dims(train_data[i], axis=0)[:, :500, :])\n",
    "                ypred.append(np.argmax(predictions, 1)[0])\n",
    "                ytrue.append(train_labels[i:i+1])\n",
    "\n",
    "\n",
    "            # The returned \"history\" object holds a record\n",
    "            # of the loss values and metric values during training\n",
    "            #print('\\nhistory dict:', history.history)\n",
    "            print(\"Epoch %d / %d  accuracy: \" % (e+1, num_epochs), end = '')\n",
    "            print(accuracy_score(np.asarray(ytrue),np.asarray(ypred)))\n",
    "            print(confusion_matrix(np.asarray(ytrue),np.asarray(ypred)))\n",
    "            ttrue = ttrue + ytrue\n",
    "            tpred = tpred + ypred\n",
    "\n",
    "        #At the end of training for num_epochs\n",
    "        print(\"Accumulated accuracy: \")\n",
    "        print(accuracy_score(np.asarray(ttrue),np.asarray(tpred)))\n",
    "        print(confusion_matrix(np.asarray(ttrue),np.asarray(tpred)))\n",
    "\n",
    "\n",
    "        test_true = []\n",
    "        test_pred = []\n",
    "        #model.load_weights(checkpoint_path.format(experiment=seed, epoch=num_epochs))\n",
    "        model.load_weights(checkpoint_path.format(experiment=seed))\n",
    "        for i in range(eval_data.shape[0]):\n",
    "\n",
    "            # Evaluate the model on the test data using `evaluate`\n",
    "            #print('\\n# Evaluate on test data')\n",
    "            #results = model.evaluate(np.expand_dims(eval_data[i], axis=0), eval_labels[i:i+1], batch_size=1)\n",
    "            #print('test loss, test acc:', results)\n",
    "\n",
    "            predictions = model.predict(np.expand_dims(eval_data[i], axis=0)[:, :500, :])\n",
    "            test_pred.append(np.argmax(predictions, 1)[0])\n",
    "            test_true.append(eval_labels[i:i+1])\n",
    "\n",
    "        print(\"Evaluation accuracy: \")\n",
    "        print(accuracy_score(np.asarray(test_true),np.asarray(test_pred)))\n",
    "        print(confusion_matrix(np.asarray(test_true),np.asarray(test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
